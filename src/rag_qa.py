import os
from dotenv import load_dotenv
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.llms import huggingface_pipeline
from langchain.chains import retrieval_qa
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, AutoConfig, GPT2Config
from lora_train import get_finedtuned_model_path
import json
from peft import PeftModel, PeftConfig
from pathlib import Path

load_dotenv()
# Lazy-load cache
_qa_pipeline = None

# def safe_load_model(model_path: str):
#     # âœ… Step 1: config.json ìˆ˜ì • í™•ì¸ ë° ë³´ì™„
#     config_path = os.path.join(model_path, "config.json")
#     if not os.path.exists(config_path):
#         raise FileNotFoundError(f"config.json not found in {model_path}")
#     with open(config_path, "r") as f:
#         config_data = json.load(f)

#     # âœ… Step 2: model_type ìžë™ ë³´ì™„ (ì˜ˆ: distilgpt2 â†’ gpt2)
#     if "model_type" not in config_data:
#         print("ðŸ”§ 'model_type' not found in config.json. Adding it manually...")
#         # ì•„ëž˜ëŠ” ì‚¬ìš©ìž ì„ íƒì— ë”°ë¼ ê³ ì¹  ìˆ˜ ìžˆìŒ
#         config_data["model_type"] = "gpt2"  # ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ì— ë”°ë¼ ë³€ê²½
#         with open(config_path, "w") as f:
#             json.dump(config_data, f)
#         print("âœ… 'model_type' successfully inserted into config.json.")

#     # âœ… Step 3: configì™€ model í•¨ê»˜ ë¡œë“œ
#     config = AutoConfig.from_pretrained(model_path)
#     model = AutoModelForCausalLM.from_pretrained(model_path, config=config)
#     return model

def get_qa_pipeline(filename: str, model_choice: str):
    global _qa_pipeline
    if _qa_pipeline is not None:
        return _qa_pipeline

    try:
        print("[DEBUG] Loading RAG pipeline")

        # model_path = get_finedtuned_model_path(filename, model_choice)
        model_path = model_path = Path(f"/tmp/lora_finetuned_model/{filename}_naive_bayes").resolve().as_posix()
        tokenizer_path = os.path.join(model_path, "_toeknizer")
        base_model_path = "/tmp/distilgpt2"
        HF_CACHE = "/tmp/hf_cache"

        print("[DEBUG] Loading tokenizer...")
        
        config = AutoConfig.from_pretrained(model_path, local_files_only=True)
        model = AutoModelForCausalLM.from_pretrained(model_path, cache_dir=HF_CACHE, config=config, local_files_only=True, trust_remote_code=True)
        
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, cache_dir=HF_CACHE, use_fast=False, local_files_only=True)
        
        # Attach LoRA adapter
        # model = PeftModel.from_pretrained(base_model, model_path, local_files_only=True)
        # model.config.model_type = "gpt2"
        # model.to("cpu")

        llm_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=200,
            do_sample=True,
            temperature=0.7,
            top_p=0.95
        )

        EMBED_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
        CHROMA_PATH = os.getenv("CHROMA_PATH", "./chroma_db")
        embedding_function = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)
        vectordb = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)

        llm = huggingface_pipeline(pipeline=llm_pipeline)
        _qa_pipeline = retrieval_qa.from_chain_type(llm=llm, retriever=vectordb.as_retriever())

        print("âœ… QA Pipeline loaded successfully.")
        return _qa_pipeline

    except Exception as e:
        print(f"âŒ Failed to load QA pipeline: {e}")
        return None
    
def run_qa(query: str, filename: str, model_choice: str) -> str:
    """
    Create a RAG QA response to a user's question
    """
    qa = get_qa_pipeline(filename, model_choice)
    response = qa.run(query)
    return response